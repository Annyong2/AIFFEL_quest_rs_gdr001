{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfa92eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07934e",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220ada0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축 파일 목록: ['korean-english-park.train.en', 'korean-english-park.train.ko']\n"
     ]
    }
   ],
   "source": [
    "# 파일 다운로드\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'korean-english-park.train.tar.gz',  # 저장될 파일명\n",
    "    origin='https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz',\n",
    "    extract=True  # 압축 해제 옵션\n",
    ")\n",
    "\n",
    "# 압축이 해제된 디렉토리 경로 확인\n",
    "extracted_dir = os.path.dirname(path_to_zip)\n",
    "\n",
    "# 압축 파일 목록 확인\n",
    "with tarfile.open(path_to_zip, \"r:gz\") as tar:\n",
    "    tar_list = tar.getnames()  # tar 파일 내 파일 목록을 가져옴\n",
    "    print(\"압축 파일 목록:\", tar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ddd9c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data Size: 94123\n",
      "Korean Data Size: 94123\n",
      "English Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n",
      "Korean Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "# tar 파일 내의 파일 경로 확인 후, 각 파일에 대한 경로 설정\n",
    "path_to_file_en = os.path.join(extracted_dir, \"korean-english-park.train.en\")\n",
    "path_to_file_ko = os.path.join(extracted_dir, \"korean-english-park.train.ko\")\n",
    "\n",
    "# 각각의 파일을 읽음\n",
    "with open(path_to_file_en, \"r\", encoding='utf-8') as f_en, open(path_to_file_ko, \"r\", encoding='utf-8') as f_ko:\n",
    "    raw_data_en = f_en.read().splitlines()\n",
    "    raw_data_ko = f_ko.read().splitlines()\n",
    "\n",
    "# 데이터 크기 및 예시 출력\n",
    "print(\"English Data Size:\", len(raw_data_en))\n",
    "print(\"Korean Data Size:\", len(raw_data_ko))\n",
    "\n",
    "# 데이터 샘플 출력 (100개 중 20개씩 출력)\n",
    "print(\"English Example:\")\n",
    "for sen in raw_data_en[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "\n",
    "print(\"Korean Example:\")\n",
    "for sen in raw_data_ko[0:100][::20]:\n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59144112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example English Sentence: Much of personal computing is about \"can you top this?\"\n",
      "Example Korean Sentence: 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 enc_corpus (영어)와 dec_corpus (한국어)에 저장\n",
    "enc_corpus = raw_data_en\n",
    "dec_corpus = raw_data_ko\n",
    "\n",
    "print(\"Example English Sentence:\", enc_corpus[0])\n",
    "print(\"Example Korean Sentence:\", dec_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b76ac0",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c340eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 초기화\n",
    "mecab = Mecab()\n",
    "\n",
    "# 1. 중복된 데이터 제거\n",
    "# 영어-한국어 병렬 데이터를 중복을 제거한 상태로 저장\n",
    "cleaned_corpus = list(set(zip(raw_data_en, raw_data_ko)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3860c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 정규식 기반 한글/영어 전처리 함수 정의\n",
    "def preprocess_sentence(sentence, lang=\"en\"):\n",
    "    if lang == \"en\":\n",
    "        # 영문 전처리 (소문자 변환, 특수문자 제거)\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", r\" \", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    else:\n",
    "        # 한글 전처리 (특수문자 제거)\n",
    "        sentence = re.sub(r\"[^가-힣0-9\\s]+\", \"\", sentence)  # 한글, 숫자, 공백 외 제거\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence).strip()    # 중복 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5204fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 영어에 <start>와 <end> 토큰 추가, 한글은 mecab 토큰화\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for eng_sentence, kor_sentence in cleaned_corpus:\n",
    "    # 영어 문장 전처리 + <start>, <end> 토큰 추가\n",
    "    eng_sentence = preprocess_sentence(eng_sentence, lang=\"en\")\n",
    "    eng_sentence = \"<start> \" + eng_sentence + \" <end>\"\n",
    "    eng_corpus.append(eng_sentence.split())  # 토큰화하여 저장\n",
    "\n",
    "    # 한국어 문장 전처리 + Mecab 토큰화\n",
    "    kor_sentence = preprocess_sentence(kor_sentence, lang=\"ko\")\n",
    "    kor_corpus.append(mecab.morphs(kor_sentence))  # Mecab을 사용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f1ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 샘플 수 (중복 제거 후): 78968\n",
      "최종 영어 코퍼스 크기: 63863\n",
      "최종 한국어 코퍼스 크기: 63863\n"
     ]
    }
   ],
   "source": [
    "# 4. 길이가 40 이하인 데이터만 선별\n",
    "final_eng_corpus = []\n",
    "final_kor_corpus = []\n",
    "\n",
    "for eng_tokens, kor_tokens in zip(eng_corpus, kor_corpus):\n",
    "    if len(eng_tokens) <= 40 and len(kor_tokens) <= 40:  # 토큰 길이가 40 이하인 경우\n",
    "        final_eng_corpus.append(eng_tokens)\n",
    "        final_kor_corpus.append(kor_tokens)\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"총 데이터 샘플 수 (중복 제거 후):\", len(cleaned_corpus))\n",
    "print(\"최종 영어 코퍼스 크기:\", len(final_eng_corpus))\n",
    "print(\"최종 한국어 코퍼스 크기:\", len(final_kor_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20caf5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example English Sentence: ['<start>', 'barcelona', 'vice', 'president', 'ferran', 'soriano', 'told', 'radio', 'marca', '<end>']\n",
      "Example Korean Sentence: ['앙리', '29', '는', '바르셀로나', '구단', '과', '4', '년', '간', '약', '299', '억', '원', '에', '계약', '하', '게', '된다']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 출력\n",
    "print(\"Example English Sentence:\", final_eng_corpus[0])\n",
    "print(\"Example Korean Sentence:\", final_kor_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8c256",
   "metadata": {},
   "source": [
    "## 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc207eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, vocab_size):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, filters='', oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# 단어의 수\n",
    "vocab_size_en = 10000\n",
    "vocab_size_ko = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c30f3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 텐서의 크기: (63863, 40)\n",
      "한국어 텐서의 크기: (63863, 40)\n"
     ]
    }
   ],
   "source": [
    "# 영어와 한국어 코퍼스를 각각 tokenize 함수에 적용\n",
    "eng_tensor, enc_tokenizer = tokenize(final_eng_corpus, vocab_size_en)\n",
    "kor_tensor, dec_tokenizer = tokenize(final_kor_corpus, vocab_size_ko)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"영어 텐서의 크기:\", eng_tensor.shape)\n",
    "print(\"한국어 텐서의 크기:\", kor_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdb31242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example English Tensor: [   4 3027  813   54    1    1   84  999    1    5    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Example Korean Tensor: [5415  621    4 2848 4817   22   94   36   97  133    1  163  226    8\n",
      " 1054   11   42  167    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 첫 번째 예시 출력\n",
    "print(\"Example English Tensor:\", eng_tensor[0])\n",
    "print(\"Example Korean Tensor:\", kor_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e992b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 tokenizer 단어 인덱스 개수: 38831\n",
      "한국어 tokenizer 단어 인덱스 개수: 39699\n"
     ]
    }
   ],
   "source": [
    "print(\"영어 tokenizer 단어 인덱스 개수:\", len(eng_tokenizer.word_index))\n",
    "print(\"한국어 tokenizer 단어 인덱스 개수:\", len(kor_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c27671",
   "metadata": {},
   "source": [
    "## 4. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c937df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee03deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dropout_rate=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)  # Dropout 추가\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.dropout(out)  # Dropout 적용\n",
    "        out, state = self.gru(out)\n",
    "        \n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18cecab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, dropout_rate=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        # context_vec 차원을 반복해서 seq_len과 맞추기\n",
    "        seq_len = tf.shape(x)[1]  # 입력 시퀀스 길이\n",
    "        context_vec = tf.expand_dims(context_vec, 1)  # [batch_size, 1, units]\n",
    "        context_vec = tf.repeat(context_vec, seq_len, axis=1)  # [batch_size, seq_len, units]\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([context_vec, out], axis=-1)  # 차원 맞춘 후 concat\n",
    "        \n",
    "        out = self.dropout(out)  # Dropout 적용\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd89dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Encoder Hidden State: (64, 1024)\n",
      "Decoder Output: (64, 39700)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "dropout_rate  = 0.3\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "\n",
    "# 인코더에서 두 가지 출력 (output과 state)을 받아줌\n",
    "sample_output, sample_state = encoder(sample_enc)\n",
    "\n",
    "# Encoder 출력 확인\n",
    "print('Encoder Output:', sample_output.shape)  # 첫 번째 반환값\n",
    "print('Encoder Hidden State:', sample_state.shape)  # 두 번째 반환값\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))  # decoder의 state 초기화\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "# Decoder 출력 확인\n",
    "print('Decoder Output:', sample_logits.shape)\n",
    "print('Decoder Hidden State:', h_dec.shape)\n",
    "print('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7d571",
   "metadata": {},
   "source": [
    "## 5. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a33b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "enc_train = eng_tensor \n",
    "dec_train = kor_tensor \n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)#학습률조정\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ce7836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 과정 정의 (EPOCHS)\n",
    "EPOCHS = 5\n",
    "\n",
    "# train_step 함수 정의 (필수)\n",
    "@tf.function\n",
    "def train_step(enc_inp, dec_inp, encoder, decoder, optimizer, dec_tokenizer):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encoder forward pass\n",
    "        enc_out, enc_hidden = encoder(enc_inp)\n",
    "\n",
    "        # Decoder 초기 상태는 인코더의 최종 hidden state로 시작\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # <start> 토큰을 Decoder의 첫 입력으로\n",
    "        dec_input = dec_inp[:, :-1]  # 마지막 <end> 토큰 제외\n",
    "        dec_target = dec_inp[:, 1:]  # 첫 <start> 토큰 제외\n",
    "\n",
    "        # 디코더를 통해 결과 예측\n",
    "        predictions, _, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # 손실 계산\n",
    "        # 패딩을 무시하도록 손실 계산\n",
    "        mask = tf.math.not_equal(dec_target, 0)  # 패딩인 부분은 False, 나머지는 True\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(dec_target, predictions, from_logits=True))\n",
    "\n",
    "    # 그라디언트 계산 및 적용\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f065a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 998/998 [04:10<00:00,  3.99it/s, Loss 4.4887]\n",
      "Epoch  2: 100%|██████████| 998/998 [04:07<00:00,  4.03it/s, Loss 4.4731]\n",
      "Epoch  3: 100%|██████████| 998/998 [04:08<00:00,  4.02it/s, Loss 4.4732]\n",
      "Epoch  4: 100%|██████████| 998/998 [04:09<00:00,  4.01it/s, Loss 4.4732]\n",
      "Epoch  5: 100%|██████████| 998/998 [04:08<00:00,  4.02it/s, Loss 4.4732]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf97d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c393d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b80be8",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b97a8",
   "metadata": {},
   "source": [
    "1. 번역기 모델 처음 만들어보는데 양쪽 언어 다 전처리하는게 좀 귀찮음\n",
    "2. loss값 너무 높게 나와서 깜짝 놀람. 이유가 뭘까\n",
    "   (1)단어 수가 너무 많아서? -> 6만개에서 한 절반만 써볼까?\n",
    "   (2)learning rate를 좀 더 낮춰볼까?(adam 기본 0.001이니 0.0001로) (3)정답 데이터에 패딩이 포함되어 있으면 손실이 많아지니까 패딩 무시해볼까?\n",
    "3. (2),(3) 적용했더니 4.6에서 4.4로 아주 조금 낮아짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
